# Анализ задачи

## 1. Что было сделано
Был создан и доработан Python-скрипт для парсинга сайта [quotes.toscrape.com](https://quotes.toscrape.com/). Основной целью было собрать информацию о цитатах и авторах, включая:
- Текст цитаты,
- Имя автора,
- Ссылку на страницу автора,
- Теги цитаты.

Дополнительно была реализована возможность интерактивного запроса, при котором пользователь может выбрать, будет ли собираться дополнительная информация о каждом авторе (биография, дата и место рождения). Все данные сохраняются в JSON-файл.

## 2. Откуда были получены данные
Все данные были получены с сайта [quotes.toscrape.com](https://quotes.toscrape.com/), который предоставляет цитаты с указанием авторов и ссылками на их страницы. На странице цитаты располагались в контейнере `<div class="quote">`, а ссылка на страницу каждого автора позволяла перейти к их биографической информации.

### Элементы для сбора:
1. **Цитата** (получена из элемента `<span class="text">`),
2. **Имя автора** (элемент `<small class="author">`),
3. **Ссылка на страницу автора** (атрибут `href` элемента `<a>`),
4. **Теги** (элементы `<a class="tag">` внутри блока `<div class="tags">`),
5. **Дополнительная информация об авторе** (заголовок, дата и место рождения, описание), доступная на персональной странице автора.

## 3. Как осуществлялся сбор
Сбор данных был организован в несколько этапов:
1. **Выбор пользователя о сборе дополнительной информации**: Добавлена функция `InputError()`, которая запрашивает у пользователя, нужно ли дополнительно собирать биографические данные об авторах. Это позволяет снизить нагрузку на сервер и ускорить сбор, если детальная информация не требуется.
2. **Цикл по страницам сайта**: Скрипт обходит страницы сайта поочередно, начиная с первой. Сбор продолжается, пока не достигается страница без цитат.
3. **Парсинг каждой страницы**: На каждой странице скрипт извлекает цитаты и основную информацию о них, включая имя автора и теги.
4. **Парсинг страницы автора (при необходимости)**: Если пользователь выбирает опцию сбора дополнительной информации, скрипт переходит на личную страницу автора и собирает биографические данные.
5. **Сохранение в JSON**: Все данные сохраняются в JSON-файл, где каждая запись структурирована с указанием информации о цитате и авторе.

## 4. Почему был выбран тот или иной метод/инструмент
- **Python**: Язык Python был выбран благодаря своей популярности и простоте для написания парсинга и работы с API, а также широкому сообществу поддержки.
- **requests**: Библиотека requests предоставляет удобные средства для отправки HTTP-запросов и загрузки веб-страниц. Она особенно полезна для статических сайтов, где страницы легко загружаются одним запросом. В данном случае requests эффективнее, чем Selenium, так как сайт не использует динамическую подгрузку данных.
- **BeautifulSoup**: Эту библиотеку удобно использовать для обработки HTML-контента и извлечения данных из нужных элементов с использованием CSS-классов и тегов. BeautifulSoup был выбран, так как он позволяет легко манипулировать и анализировать HTML-структуры страниц.
- **JSON**: Формат JSON был выбран для сохранения данных благодаря своей читабельности и простоте интеграции с различными приложениями. JSON также позволяет легко перебирать данные и анализировать их, если потребуется.

Эта структура скрипта обеспечивает гибкость и эффективность при сборе данных, позволяя пользователю выбирать нужные данные, что делает программу более производительной и адаптируемой для различных целей.
